{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Spark\n",
    "\n",
    "The Spark **R**esilient **D**istributed **D**ataset API allows us to specify data processing operations, which when executed get distributed across multiple workers.\n",
    "\n",
    "A Resilient Distributed Dataset (RDD) represents an immutable collection of elements that can be operated on in parallel.\n",
    "\n",
    "We interact with the Spark cluster by submitting queries via the Python `SparkContext` object.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "Most methods on the SparkContext are lazy.  In other words, the `filter` method itself doesn't perform any processing and is very fast to call. Calling `collect`, however, will submit a `Job` to the cluster and wait for it to complete before printing results back to Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Examples\n",
    "\n",
    "- [spark_context.parallelize](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html#pyspark.SparkContext.parallelize) - Form an RDD from a Python list.\n",
    "- [filter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter) - Return a new RDD containing only the elements that satisfy a predicate.\n",
    "- [collect](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect) - Return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark_context = SparkContext.getOrCreate()\n",
    "rdd = spark_context.parallelize([\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Waltz, bad nymph, for quick jigs vex\",\n",
    "    \"Pack my box with five dozen liquor jugs\"\n",
    "])\n",
    "\n",
    "# Which pangrams are less than 40 characters in length?\n",
    "rdd.filter(lambda pangram: len(pangram) < 40).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) - Return a new RDD by applying a function to each element of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda pangram: pangram[::-1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [count](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count) - Return the number of elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda pangram: 'fox' in pangram or 'box' in pangram).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert the existing pangrams from the `rdd` variable to a list of title case pangrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate from the `rdd` variable, a list of pangrams which contain only alpha and space characters.  In other words, don't include pangrams containing punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate from the `rdd` variable, a sorted list of pangrams from longest to shortest.\n",
    "    <details>\n",
    "      <summary>Hint</summary>\n",
    "      Look at the sortBy RDD method within the API docs, see Resources below.\n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Imagine `rdd` contains thousands of pangrams, but you'd like to look at just two of them.\n",
    "    <details>\n",
    "      <summary>Hint</summary>\n",
    "      Search for \"take\" RDD methods within the API docs, see Resources below.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [Spark RDD API docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b664d7538e9e6cb83f68b34f1169bfb1b78da0986d7cdad8b8d2121e82a8c8fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
