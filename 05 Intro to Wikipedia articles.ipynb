{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Wikipedia Articles\n",
    "\n",
    "Thankfully, the Wikipedia Foundation provides under [their license](https://creativecommons.org/licenses/by-sa/3.0/) free access to their full encylopedia of knowledge via their [dumps website](https://dumps.wikimedia.org/).\n",
    "\n",
    "The work of downloading the data and [processing it into a usable form](https://github.com/attardi/wikiextractor) into an S3 bucket has already been performed for you.\n",
    "\n",
    "- [read.json](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html) - Loads JSON files and returns the results as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "wikipedia_data = spark_session.read.format(\"json\").json(\"wikipedia-dump/*/wiki_*\")\n",
    "# Calling count below will cause Spark to process the downloaded Wikidata\n",
    "# You can navigate to the terminal where make is being run to view a progress bar\n",
    "wikipedia_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [limit](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html#pyspark.sql.DataFrame.limit) - Limits the result count to the number specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 10 Wikipedia articles took < 1second during testing.\n",
    "wikipedia_data.select('title').limit(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [printSchema](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html) - Prints out the schema in the tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. How many articles have a zero length? What's that as a percentage of the total article count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the [Soundex](https://en.wikipedia.org/wiki/Soundex) algorithm to find article titles which sound like \"Adrian\".\n",
    "    <details>\n",
    "      <summary>Hint</summary>\n",
    "      To operate on a constant value, e.g. Adrian you can use the pyspark.sql.functions.lit function\n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the number of revisions a wikipedia article has and the length of the article?\n",
    "    <details>\n",
    "      <summary>Hint 1</summary>\n",
    "      The DataFrame object has a method corr which can calculate this.\n",
    "    </details>\n",
    "    <details>\n",
    "      <summary>Hint 2</summary>\n",
    "      The revid column will need to be cast to an integer.\n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the most frequently occurring 5 letter word across all articles?\n",
    "    <details>\n",
    "      <summary>Hint 1</summary>\n",
    "      Creating a DataFrame which has a column containing an array of all words within the article could help.\n",
    "    </details>\n",
    "    <details>\n",
    "    <summary>Hint 2</summary>\n",
    "      Manipulating your array of words is hard, but transforming that into a row per word should make filtering and aggregating easier.\n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How does the frequency of letters within Wikipedia compare to [The frequency of the letters of the alphabet in English](https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- List of [pyspark.sql.functions](https://spark.apache.org/docs/3.1.3/api/python/reference/pyspark.sql.html#functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b664d7538e9e6cb83f68b34f1169bfb1b78da0986d7cdad8b8d2121e82a8c8fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
